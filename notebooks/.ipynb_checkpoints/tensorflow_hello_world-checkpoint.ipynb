{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Introduction\n",
    "## [source](https://textminingonline.com/dive-into-tensorflow-part-i-getting-started-with-tensorflow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: there is actually problems with edward due to some methods errors.   \n",
    "   \n",
    "Installation of tensorflow:\n",
    "`sudo pip3 install tensorflow`\n",
    "\n",
    "Installation of edward:\n",
    "`sudo pip3 install edward`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'set_shapes_for_outputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-20a1cb44dbde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Test of edward import:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0medward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/edward/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0medward\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcriticisms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0medward\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minferences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0medward\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/edward/criticisms/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0medward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriticisms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0medward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriticisms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mppc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0medward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriticisms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mppc_plots\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/edward/criticisms/evaluate.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0medward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0medward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_multinomial_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mwith_binary_averaging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/edward/util/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0medward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0medward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0medward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_variables\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0medward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/edward/util/random_variables.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0medward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraphs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom_variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'set_shapes_for_outputs'"
     ]
    }
   ],
   "source": [
    "#Test of edward import:\n",
    "import edward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a known issue: see [that thread](https://github.com/blei-lab/edward/issues/882). Fix is to use a older version of TF. But first let's continue getting familiarized with TF.\n",
    "\n",
    "## 1) First program: [Hello World!](https://textminingonline.com/dive-into-tensorflow-part-i-getting-started-with-tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, TensorFlow!\n"
     ]
    }
   ],
   "source": [
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello).decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should get `Hello, TensorFlow!`, there we get added characters. I don't know why.   \n",
    "Update - Reason: bytes unicode stuff, to see the `b'text_to_be_printed'` disappear, add `.decode()`.   \n",
    "(the b'...' stands for literals bytes, need to be decoded in UTF8, thus the decode function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(10)\n",
    "b = tf.constant(100)\n",
    "op = tf.add(a,b)\n",
    "print(sess.run(op))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "c = a * b\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c))\n",
    "    print(c.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.InteractiveSession at 0x1c39d12128>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2.]\n",
      "[3. 3.]\n"
     ]
    }
   ],
   "source": [
    "a = tf.zeros((2,2)); b = tf.ones((2,3))\n",
    "# b= [1 1 1]  a= 0 0\n",
    "#    [1 1 1]     0 0\n",
    "print(tf.reduce_sum(b,0).eval()) # sum of columns of b\n",
    "print(tf.reduce_sum(b,1).eval()) # sum of rows of b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(2), Dimension(2)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(a, (1,4)).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic concepts\n",
    "## What is a Tensor :\n",
    "### A Tensor is list of data\n",
    "\n",
    ">TensorFlow programs use a tensor data structure to represent all data — only tensors are passed between operations in the computation graph. You can think of a TensorFlow tensor as an n-dimensional array or list.\n",
    "\n",
    "Tensor dimensionality:\n",
    "- **Rank**: number of dimensions   \n",
    "0: scalar   \n",
    "1: vector   \n",
    "2: matrix   \n",
    "3: _cube_ like, etc\n",
    "- **Shape**\n",
    "- **Dimension number**\n",
    "In addition to rank, shape and dimension number, Tensors have data type.\n",
    "\n",
    "| Data type \t| Python type \t| Description \t|\n",
    "|---------------\t|:-------------:\t|:-----------------------------------------------------------------------------:\t|\n",
    "| DT_FLOAT \t| tf.float32 \t| 32 bits floating point. \t|\n",
    "| DT_DOUBLE \t| tf.float64 \t| 64 bits floating point. \t|\n",
    "| DT_INT8 \t| tf.int8 \t| 8 bits signed integer. \t|\n",
    "| DT_INT16 \t| tf.int16 \t| 16 bits signed integer. \t|\n",
    "| DT_INT32 \t| tf.int32 \t| 32 bits signed integer. \t|\n",
    "| DT_INT64 \t| tf.int64 \t| 64 bits signed integer. \t|\n",
    "| DT_UINT8 \t| tf.uint8 \t| 8 bits unsigned integer. \t|\n",
    "| DT_STRING \t| tf.string \t| Variable length byte arrays. Each element of a Tensor is a byte array. \t|\n",
    "| DT_BOOL \t| tf.bool \t| Boolean. \t|\n",
    "| DT_COMPLEX64 \t| tf.complex64 \t| Complex number made of two 32 bits floating points: real and imaginary parts. \t|\n",
    "| DT_COMPLEX128 \t| tf.complex128 \t| Complex number made of two 64 bits floating points: real and imaginary parts. \t|\n",
    "| DT_QINT8 \t| tf.qint8 \t| 8 bits signed integer used in quantized Ops. \t|\n",
    "| DT_QINT32 \t| tf.qint32 \t| 32 bits signed integer used in quantized Ops. \t|\n",
    "| DT_QUINT8 \t| tf.quint8 \t| 8 bits unsigned integer used in quantized Ops. \t|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scalar: rank 0 shape []\n",
    "s1 = tf.constant(100)\n",
    "s1.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(5)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector: rank 1 shape [D0]\n",
    "v1 = tf.constant([1,2,3,4,5]) # list of 5 elements, matrix 1x5\n",
    "v1.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(2), Dimension(3)])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix: rank 2 shape [D0, D1]\n",
    "m1 = tf.constant([[1,2,3],[4,5,6]]) # list of 2 lists of 3 elements, matrix 2x3\n",
    "m1.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(3), Dimension(3), Dimension(1)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3-tensor (cube): rank 3 shape [D0, D1, D2]\n",
    "c1 = tf.constant([[[1], [2], [3]], [[4], [5], [6]], [[7], [8], [9]]])\n",
    "# list of 3 lists of list of 1 element, matrix 3x3\n",
    "c1.get_shape()\n",
    "# 3 rows, 3 columns, 1 elements per list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(3), Dimension(3), Dimension(2)])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3-tensor (cube): rank 3 shape [D0, D1, D2]\n",
    "c1 = tf.constant([[[1,2], [2,3], [3,4]], [[4,5], [5,6], [6,7]], [[7,8], [8,9], [9,10]]])\n",
    "# list of 3 lists of list of 2 element, matrix 3x3\n",
    "c1.get_shape()\n",
    "# 3 rows, 3 columns, 2 elements per list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "> In TensorFlow, variables maintain state across executions of the graph. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "0: 1000\n",
      "1: 10000\n",
      "2: 100000\n",
      "3: 1000000\n",
      "4: 10000000\n"
     ]
    }
   ],
   "source": [
    "# Create a Variable, that will be initialized to the scalar value 100\n",
    "var = tf.Variable(100, name=\"variable_counter\")\n",
    "\n",
    "# Create an Op to multiply ten to `var`.\n",
    "ten = tf.constant(10)\n",
    "\n",
    "new_var = tf.multiply(var, ten)\n",
    "\n",
    "update = tf.assign(var, new_var)\n",
    "\n",
    "\n",
    "# Variables must be initialized by running an `init` Op after having\n",
    "# launched the graph.  We first have to add the `init` Op to the graph.\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph and run the ops.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    print(sess.run(var))\n",
    "    for i in range(5):\n",
    "        sess.run(update)\n",
    "        print(str(i) + ': ' + str(sess.run(var)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([20.], dtype=float32), array([5.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a variable with a random value.\n",
    "weights = tf.Variable(tf.random_normal([4096, 500], stddev=0.25), name=\"weights\")\n",
    "#print(weights.eval())\n",
    "\n",
    "# Create another variable with the same value as 'weights'.\n",
    "weights2 = tf.Variable(weights.initialized_value(), name=\"weights2\")\n",
    "#print(weights2.eval())\n",
    "\n",
    "# Create another variable with twice the value of 'weights'\n",
    "weights_twice = tf.Variable(weights.initialized_value() * 2.0, name=\"weights_twice\")\n",
    "#print(weights_twice.eval())\n",
    "\n",
    "constant1 = tf.constant([4.0]) #edge\n",
    "\n",
    "constant2 = tf.constant([2.0]) #edge\n",
    "\n",
    "constant3 = tf.constant([3.0]) #edge\n",
    "\n",
    "intermed = tf.add(constant2, constant3)  # node: add c2 and c3\n",
    "\n",
    "mul = tf.multiply(constant1, intermed)  # node: multiply c1 and results of node intermed\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    result = sess.run([mul, intermed])\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feeds\n",
    "\n",
    ">TensorFlow provides a feed mechanism for patching a tensor directly into any operation in the graph.   \n",
    "A feed temporarily replaces the output of an operation with a tensor value. You supply feed data as an argument to a run() call. The feed is only used for the run call to which it is passed. The most common use case involves designating specific operations to be “feed” operations by using tf.placeholder() to create them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([20.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "pla1 = tf.placeholder(tf.float32)\n",
    "\n",
    "pla2 = tf.placeholder(tf.float32)\n",
    "\n",
    "result = tf.multiply(pla1, pla2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([result], feed_dict={pla1:[4.], pla2:[5.]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(100)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    for i in range(100):\n",
    "        value = sess.run(next_element)\n",
    "        print(value)\n",
    "        assert i == value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "# ex.csv\n",
    "#        1, 2,  3,  4\n",
    "#        5, 6,  7,  8\n",
    "#        9, 10, 11, 12\n",
    "\n",
    "from random import shuffle\n",
    "#filenames = \"ex.csv\"\n",
    "ncol = 0\n",
    "#with open(filenames,\"r\") as input:\n",
    "#    ncol = len(input.readline().split(\",\"))\n",
    "#print(ncol)\n",
    "#col_list = list(range(ncol))\n",
    "#shuffle(col_list)\n",
    "#col_list = tf.data.Dataset.range(ncol).shuffle(buffer_size=ncol)\n",
    "#col_list = tf.range(ncol)\n",
    "#col_list = tf.random_shuffle(col_list)\n",
    "#iterator = col_list.make_one_shot_iterator()\n",
    "#next_element = iterator.get_next()\n",
    "#record_defaults = [tf.int32]\n",
    "#for current_column in col_list:\n",
    "column = tf.contrib.data.CsvDataset(\n",
    "                    \"ex.csv\", \n",
    "                    [tf.int32],\n",
    "                    #header = False, \n",
    "                    select_cols = [1]\n",
    ")\n",
    "                    #select_cols = [current_column]) #iterator to get 1 random column at a time\n",
    "next_elem = column.make_one_shot_iterator().get_next()\n",
    "with tf.Session() as sess:\n",
    "       # while True:\n",
    "         #   try:\n",
    "                #col_list = sess.run(tf.random_shuffle(col_list))\n",
    "\n",
    "                    #current_column = [sess.run(next_element)]\n",
    "    #print(\"Current column of the csv: \" + str(current_column))\n",
    "                    #print(current_column)\n",
    "                    \n",
    "    #print(sess.run(column.make_one_shot_iterator().get_next()))\n",
    "    print(sess.run(next_elem))\n",
    "\n",
    "\n",
    "                #mean = tf.reduce_mean(column)\n",
    "            #print(sess.run(mean.eval()))\n",
    "            \n",
    "\n",
    "            \n",
    "       # except tf.errors.OutOfRangeError:\n",
    "          #  break\n",
    "    \n",
    "\n",
    "\n",
    "# whhhhyyyyyyyyy fknshitfuk\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "            try:\n",
    "                print(sess.run(next_element))\n",
    "\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n",
      "(5,)\n",
      "(9,)\n",
      "(2,)\n",
      "(6,)\n",
      "(10,)\n",
      "(3,)\n",
      "(7,)\n",
      "(11,)\n",
      "(4,)\n",
      "(8,)\n",
      "(12,)\n"
     ]
    }
   ],
   "source": [
    "# dev double iter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    dataset = tf.contrib.data.CsvDataset(\n",
    "      \"ex.csv\",\n",
    "      #[tf.float32,  # Required field, use dtype or empty tensor\n",
    "      #tf.constant([0.0], dtype=tf.float32),  # Optional field, default to 0.0\n",
    "      #tf.int32,  # Required field, use dtype or empty tensor\n",
    "       [tf.int32],\n",
    "      select_cols=[i]  # Only parse last three columns\n",
    "    )\n",
    "\n",
    "    next_element = dataset.make_one_shot_iterator().get_next()\n",
    "    with tf.Session() as sess:\n",
    "        while True:\n",
    "            try:\n",
    "                print(sess.run(next_element))\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n",
      "(5,)\n",
      "(9,)\n",
      "(2,)\n",
      "(6,)\n",
      "(10,)\n",
      "(3,)\n",
      "(7,)\n",
      "(11,)\n",
      "(4,)\n",
      "(8,)\n",
      "(12,)\n"
     ]
    }
   ],
   "source": [
    "# THAT CODE WORKS - DON'T TOUCH IT, WORK ON COPY\n",
    "\n",
    "for i in range(4):\n",
    "    dataset = tf.contrib.data.CsvDataset(\n",
    "      \"ex.csv\",\n",
    "      #[tf.float32,  # Required field, use dtype or empty tensor\n",
    "      #tf.constant([0.0], dtype=tf.float32),  # Optional field, default to 0.0\n",
    "      #tf.int32,  # Required field, use dtype or empty tensor\n",
    "       [tf.int32],\n",
    "      select_cols=[i]  # Only parse last three columns\n",
    "    )\n",
    "\n",
    "    next_element = dataset.make_one_shot_iterator().get_next()\n",
    "    with tf.Session() as sess:\n",
    "        while True:\n",
    "            try:\n",
    "                print(sess.run(next_element))\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('unable to open database file',)).History will not be written to the database.Traceback (most recent call last):\n",
      "  File \"/Users/admin/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-95-d60f3679b461>\", line 5, in <module>\n",
      "    csv_in = open(\"ex.csv\", \"r\")                        # open the csv\n",
      "OSError: [Errno 24] Too many open files: 'ex.csv'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/admin/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1828, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/admin/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/admin/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/admin/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/admin/anaconda3/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/admin/anaconda3/lib/python3.6/inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/admin/anaconda3/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/admin/anaconda3/lib/python3.6/inspect.py\", line 725, in getmodule\n",
      "    file = getabsfile(object, _filename)\n",
      "  File \"/Users/admin/anaconda3/lib/python3.6/inspect.py\", line 709, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "  File \"/Users/admin/anaconda3/lib/python3.6/posixpath.py\", line 374, in abspath\n",
      "    cwd = os.getcwd()\n",
      "OSError: [Errno 24] Too many open files\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 24] Too many open files: 'ex.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread IPythonHistorySavingThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/admin/anaconda3/lib/python3.6/site-packages/IPython/core/history.py\", line 834, in run\n",
      "    self.history_manager.writeout_cache(self.db)\n",
      "  File \"<decorator-gen-23>\", line 2, in writeout_cache\n",
      "  File \"/Users/admin/anaconda3/lib/python3.6/site-packages/IPython/core/history.py\", line 58, in needs_sqlite\n",
      "    return f(self, *a, **kw)\n",
      "  File \"/Users/admin/anaconda3/lib/python3.6/site-packages/IPython/core/history.py\", line 780, in writeout_cache\n",
      "    self._writeout_input_cache(conn)\n",
      "  File \"/Users/admin/anaconda3/lib/python3.6/site-packages/IPython/core/history.py\", line 764, in _writeout_input_cache\n",
      "    (self.session_number,)+line)\n",
      "sqlite3.OperationalError: unable to open database file\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/admin/anaconda3/lib/python3.6/site-packages/ipykernel/iostream.py\", line 97, in _event_pipe\n",
      "    event_pipe = self._local.event_pipe\n",
      "AttributeError: '_thread._local' object has no attribute 'event_pipe'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/admin/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"<decorator-gen-24>\", line 2, in run\n",
      "  File \"/Users/admin/anaconda3/lib/python3.6/site-packages/IPython/core/history.py\", line 58, in needs_sqlite\n",
      "    return f(self, *a, **kw)\n",
      "  File \"/Users/admin/anaconda3/lib/python3.6/site-packages/IPython/core/history.py\", line 837, in run\n",
      "    \"History will not be written to the database.\") % repr(e))\n",
      "  File \"/Users/admin/anaconda3/lib/python3.6/site-packages/ipykernel/iostream.py\", line 376, in write\n",
      "    self.pub_thread.schedule(lambda : self._buffer.write(string))\n",
      "  File \"/Users/admin/anaconda3/lib/python3.6/site-packages/ipykernel/iostream.py\", line 203, in schedule\n",
      "    self._event_pipe.send(b'')\n",
      "  File \"/Users/admin/anaconda3/lib/python3.6/site-packages/ipykernel/iostream.py\", line 101, in _event_pipe\n",
      "    event_pipe = ctx.socket(zmq.PUSH)\n",
      "  File \"/Users/admin/anaconda3/lib/python3.6/site-packages/zmq/sugar/context.py\", line 146, in socket\n",
      "    s = self._socket_class(self, socket_type, **kwargs)\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 285, in zmq.backend.cython.socket.Socket.__cinit__\n",
      "zmq.error.ZMQError: Too many open files\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "# Get the numbers of columns in the csv:\n",
    "\n",
    "csv_in = open(\"ex.csv\", \"r\")                        # open the csv\n",
    "ncol = len(csv_in.readline().split(\",\"))            # read the first line and count the # of columns\n",
    "csv_in.close()                                      # close the csv\n",
    "print(\"Number of columns in the csv: \" + str(ncol)) # print the # of columns\n",
    "\n",
    "# Create a random column reading list:\n",
    "\n",
    "col_list = list(range(ncol))\n",
    "shuffle(col_list)\n",
    "\n",
    "#col_list = tf.data.Dataset.range(ncol).shuffle(buffer_size=ncol)\n",
    "#next_col = col_list.make_one_shot_iterator().get_next()\n",
    "#next_col_iter = col_list.make_initializable_iterator()\n",
    "\n",
    "#dataset = tf.contrib.data.CsvDataset(\n",
    "#    \"ex.csv\",\n",
    "#    [tf.int64],\n",
    "#    select_cols=[next_col]  # Only parse last three columns\n",
    "#)\n",
    "\n",
    "#next_element = dataset.make_one_shot_iterator().get_next()\n",
    "with tf.Session() as sess:\n",
    "    while True:\n",
    "        try:\n",
    "            for current_column in col_list:\n",
    "                dataset = tf.contrib.data.CsvDataset(\n",
    "\n",
    "                    \"ex.csv\",\n",
    "                    [tf.int64],\n",
    "                    select_cols=[current_column]  # Only parse current_column\n",
    "                )\n",
    "\n",
    "                next_element = dataset.make_one_shot_iterator().get_next()\n",
    "                print(sess.run(next_element))\n",
    "                \n",
    "            \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in the csv: 4\n",
      "Current column: 0\n",
      "(1,)\n",
      "(5,)\n",
      "(9,)\n",
      "Current column: 2\n",
      "(3,)\n",
      "(7,)\n",
      "(11,)\n",
      "Current column: 1\n",
      "(2,)\n",
      "(6,)\n",
      "(10,)\n",
      "Current column: 3\n",
      "(4,)\n",
      "(8,)\n",
      "(12,)\n"
     ]
    }
   ],
   "source": [
    "# WORKING VERSION\n",
    "\n",
    "from random import shuffle\n",
    "#import tensorflow as tf\n",
    "\n",
    "# Get the numbers of columns in the csv:\n",
    "\n",
    "csv_in = open(\"ex.csv\", \"r\")                        # open the csv\n",
    "ncol = len(csv_in.readline().split(\",\"))            # read the first line and count the # of columns\n",
    "csv_in.close()                                      # close the csv\n",
    "print(\"Number of columns in the csv: \" + str(ncol)) # print the # of columns\n",
    "\n",
    "# Create a random column reading list:\n",
    "col_list = list(range(ncol))\n",
    "shuffle(col_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    for i in col_list:\n",
    "        dataset = tf.contrib.data.CsvDataset(\n",
    "            \"ex.csv\",\n",
    "            [tf.int32],\n",
    "            select_cols=[i]  # Only parse last three columns\n",
    "        )\n",
    "        next_element = dataset.make_one_shot_iterator().get_next()\n",
    "        print('Current column: ' + str(i))\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                print(sess.run(next_element))\n",
    "                \n",
    "\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                \n",
    "                \n",
    "                break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To keep in mind:\n",
    "\n",
    "NOTE: It is legitimate to call Iterator.get_next() multiple times, e.g. when you are distributing different elements to multiple devices in a single step. __However, a common pitfall arises when users call Iterator.get_next() in each iteration of their training loop. Iterator.get_next() adds ops to the graph, and executing each op allocates resources (including threads); as a consequence, invoking it in every iteration of a training loop causes slowdown and eventual resource exhaustion.__ To guard against this outcome, we log a warning when the number of uses crosses a fixed threshold of suspiciousness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in the csv: 4\n",
      "0\n",
      "(1.0,)\n",
      "(5.0,)\n",
      "(9.0,)\n",
      "3\n",
      "(4.0,)\n",
      "(8.0,)\n",
      "(12.0,)\n",
      "2\n",
      "(3.0,)\n",
      "(7.0,)\n",
      "(11.0,)\n",
      "1\n",
      "(2.0,)\n",
      "(6.0,)\n",
      "(10.0,)\n"
     ]
    }
   ],
   "source": [
    "# WORKING VERSION\n",
    "\n",
    "#from random import shuffle\n",
    "#import tensorflow as tf\n",
    "\n",
    "# Get the numbers of columns in the csv:\n",
    "\n",
    "csv_in = open(\"ex.csv\", \"r\")                        # open the csv\n",
    "ncol = len(csv_in.readline().split(\",\"))            # read the first line and count the # of columns\n",
    "csv_in.close()                                      # close the csv\n",
    "print(\"Number of columns in the csv: \" + str(ncol)) # print the # of columns\n",
    "\n",
    "# Create a random column reading list:\n",
    "#col_list = list(range(ncol))\n",
    "#shuffle(col_list)\n",
    "\n",
    "\n",
    "\n",
    "col_list = tf.data.Dataset.range(ncol).shuffle(buffer_size=ncol)\n",
    "col_next = col_list.make_one_shot_iterator().get_next()\n",
    "\n",
    "#x = tf.placeholder(float32)\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            index = sess.run(col_next)\n",
    "            dataset = tf.contrib.data.CsvDataset(\n",
    "                        \"ex.csv\",\n",
    "                        [tf.float32],\n",
    "                        select_cols=[index]  # Only parse last three columns\n",
    "                    )\n",
    "            next_element = dataset.make_one_shot_iterator().get_next()\n",
    "            print(index)\n",
    "            while True: \n",
    "                try:\n",
    "                    print(sess.run(next_element))\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
